import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import sklearn
from imblearn.over_sampling import RandomOverSampler
from nltk.corpus import wordnet
import nltk
from nltk.tag.perceptron import PerceptronTagger
from nltk.stem import WordNetLemmatizer
from tqdm import tqdm
import re
import string
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import regularizers
from tensorflow import keras
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout,Softmax,Activation, Dense, Embedding, GlobalAveragePooling1D,Bidirectional,LSTM
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import CategoricalAccuracy
from tensorflow.keras.callbacks import EarlyStopping
!pip install keras-tuner --upgrade
import keras_tuner as kt
from keras.src.layers.serialization import activation
from tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
!pip install keras_self_attention
from keras_self_attention import SeqSelfAttention
!pip install -U fasttext
import fasttext.util

text_message=pd.read_csv('spam_text_message.csv')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
target=text_message.pop('Category')

mapping={'ham':0,'spam':1}
target=target.map(lambda x: mapping.get(x,x))

X_train,X_test,y_train,y_test=train_test_split(text_message,target,test_size=0.25,random_state=42)    
ros = RandomOverSampler(random_state=42,sampling_strategy=1)
#X_res, y_res = ros.fit_resample(text_message, target)

text_train=[]
for x in X_train['Message']:
    text_train.append(x)
    
text_test=[]
for x in X_test['Message']:
    text_test.append(x)

#clean text
def pos_tag(x):
    if x.startswith('J'):
        return wordnet.ADJ
    elif x.startswith('V'):
        return wordnet.VERB
    elif x.startswith('N'):
        return wordnet.NOUN
    elif x.startswith('R'):
        return wordnet.ADV
    else: 
        return None


def clean_body(input):
    lemmatizer=WordNetLemmatizer()
    tagger=PerceptronTagger()
    docs=[]


    for idx in tqdm(range(len(input))):
        review=re.sub(r'\W',' ',str(input[idx]))
        review=re.sub(r'\s+[a-zA-Z]\s+',' ',review)
        review=re.sub(r'\s+',' ',review,flags=re.I)
        review=re.sub(r'\d',' ',review)
        review=re.sub(r'\b\d+\d',' ',review)
        review=review.lower()
        review=review.split()

        pos=[x[1] for x in tagger.tag(review)]

        review=\
        [lemmatizer.lemmatize(token,pos=pos_tag(tag))\
        if pos_tag(tag)!=None else lemmatizer.lemmatize(token)\
        for token,tag in zip(review,pos)]
        review=' '.join(review)
        docs.append(review)
        
    return docs
 
docs_train=clean_body(text_train)
docs_test=clean_body(text_test)


def clean_body(input):
    lemmatizer=WordNetLemmatizer()
    tagger=PerceptronTagger()
    docs=[]


    for idx in tqdm(range(len(input))):
        review=re.sub(r'\W',' ',str(input[idx]))
        review=re.sub(r'\s+[a-zA-Z]\s+',' ',review)
        review=re.sub(r'\s+',' ',review,flags=re.I)
        review=re.sub(r'\d',' ',review)
        review=re.sub(r'\b\d+\d',' ',review)
        review=review.lower()
        review=review.split()

        pos=[x[1] for x in tagger.tag(review)]

        review=\
        [lemmatizer.lemmatize(token,pos=pos_tag(tag))\
        if pos_tag(tag)!=None else lemmatizer.lemmatize(token)\
        for token,tag in zip(review,pos)]
        review=' '.join(review)
        docs.append(review)
        
    return docs
 
docs_train=clean_body(text_train)
docs_test=clean_body(text_test)

#tf-idf
vectorizer=TfidfVectorizer(ngram_range=(1,3),max_features=4000,sublinear_tf=True)

X_train_tfidf=vectorizer.fit_transform(docs_train)
X_test_tfidf=vectorizer.transform(docs_test)

#truncatedSVD
svd=TruncatedSVD(n_components=400,random_state=42)
x_train_svd=svd.fit_transform(X_train_tfidf)
x_test_svd=svd.transform(X_test_tfidf)

normalizer=keras.layers.Normalization()
normalizer.adapt(x_train_svd)

#using Neural Networks
p0,p1=target.value_counts()/target.count()
initial_bias=np.log(p1/p0)

model=Sequential([
    Dense(8,input_dim=x_train_svd.shape[1],activation='elu'),
    Dense(4,activation='elu'),
    Dense(1,activation='sigmoid',bias_initializer=tf.keras.initializers.Constant(initial_bias))
])

early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,
                                             mode='max',restore_best_weights=True)

model.summary()

METRICS=[
    keras.metrics.BinaryAccuracy(name='accuracy'),
    keras.metrics.Precision(name='precision'),
    keras.metrics.Recall(name='recall'),
    keras.metrics.AUC(name='prc', curve='PR'),
    keras.metrics.AUC(name='roc', curve='ROC')
]
model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),
              loss=tf.keras.losses.BinaryCrossentropy(),metrics=METRICS)

results=model.evaluate(x_test_svd,y_test)
expected_loss = - (p0 * np.log(p0) + p1 * np.log(p1))
print(f'Expected loss:{expected_loss:.4f}')
print(f'Loss (before training): {results[0]}')

history=model.fit(x_train_svd,y_train,validation_split=0.2,callbacks=[early_stop],epochs=40)

test_loss,test_acc,test_precision,test_recall,test_pr_curve,test_recall_curve=model.evaluate(x_test_svd,y_test)
print('Test accuracy:', test_acc)
print('Test precision:', test_precision)
print('Test recall:', test_recall)
print('Test PR curve:', test_pr_curve)
print('Test ROC curve:',test_recall_curve)

def plot_loss(history):
    plt.plot(history.history['loss'],label='train_loss')
    plt.plot(history.history['val_loss'],label='val_loss')
    plt.grid(linestyle='--')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.ylim([0,0.5])
    plt.legend()
    
plot_loss(history)

METRICS=[
    keras.metrics.BinaryAccuracy(name='accuracy'),
    keras.metrics.Precision(name='precision'),
    keras.metrics.Recall(name='recall'),
    keras.metrics.AUC(name='prc', curve='PR'),
    keras.metrics.AUC(name='roc', curve='ROC')
]
#grid search for NNs
def build_model(hp):
    model=Sequential()
    layer_index=0
    for i in range(hp.Int(name='num_layers',min_value=1,max_value=4)):
        if layer_index==0:
            model.add(Dense(hp.Int(name='hidden_units_'+str(i),min_value=2,max_value=10,step=2),
                           activation=hp.Choice(name='activation_layer'+str(i),values=['relu','elu']),
                           input_dim=x_train_svd.shape[1]))
            model.add(Dropout(hp.Choice(name='dropout_layer_'+str(i),values=[0.0,0.2,0.4])))
        else:
            model.add(Dense(hp.Int(name='hidden_units_'+str(i),min_value=2,max_value=10,step=2),
                            activation=hp.Choice(name='activation_layer_'+str(i),values=['relu','elu'])))
            model.add(Dropout(hp.Choice(name='dropout_layer_'+str(i),values=[0.0,0.2,0.4])))
            layer_index+=1
            
        model.add(Dense(1,activation='sigmoid',bias_initializer=tf.keras.initializers.Constant(initial_bias)))
        
        hp_learning_rate=hp.Choice('learning_rate',values=[1e-2,1e-3])
        model.compile(optimizer=tf.keras.optimizers.Adam(hp_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(),metrics=METRICS)
        
        return model

tuner = kt.BayesianOptimization(build_model,
                        objective=kt.Objective("val_accuracy", direction="max"),
                        max_trials = 20,
                        directory='KT_dir',
                        project_name='KT_tuning',
                        overwrite=True)

early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,
                                             mode='max',restore_best_weights=True)
tuner.search_space_summary()

tuner.search(x_train_svd, y_train, 
             validation_split=0.2, epochs=40, batch_size =32,callbacks=[early_stop])

best_model=tuner.get_best_models(num_models=1)[0]
best_model.summary()
history=best_model.fit(x_train_svd, y_train, 
             validation_split=0.2, epochs=40, batch_size =32,callbacks=[early_stop])

test_loss,test_acc,test_precision,test_recall,test_pr_curve,test_recall_curve=best_model.evaluate(x_test_svd,y_test)
print('Test accuracy:', test_acc)
print('Test precision:', test_precision)
print('Test recall:', test_recall)
print('Test PR curve:', test_pr_curve)
print('Test ROC curve:',test_recall_curve)


plot_loss(history)

# Neural Networks using embeddings


batch_size=32
embedding_dim=512
vocab_size=5000
sequence_length=400

tf_train=tf.data.Dataset.from_tensor_slices((docs_train,y_train)).batch(batch_size)
tf_test=tf.data.Dataset.from_tensor_slices((docs_test,y_test)).batch(batch_size)


vectorize_layer=TextVectorization(max_tokens=vocab_size,input_shape=(1,),
                                  output_mode='int',output_sequence_length=sequence_length)

train_text=tf_train.map(lambda x,y: x)
vectorize_layer.adapt(train_text)

tf_train=tf_train.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
tf_test=tf_test.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)


validation_fraction = 0.25  # Adjust as needed
num_samples = tf.data.experimental.cardinality(tf_train).numpy()
num_validation_samples = int(validation_fraction * num_samples)

# Create a validation dataset
tf_validation = tf_train.take(num_validation_samples)
tf_train = tf_train.skip(num_validation_samples)


model=tf.keras.Sequential([
    vectorize_layer,
    Embedding(input_dim=vocab_size,output_dim=embedding_dim,name='embedding'),
    Dense(32,activation='elu'),
    Dense(16,activation='elu'),
    GlobalAveragePooling1D(),
    Dense(1,activation='sigmoid',bias_initializer=tf.keras.initializers.Constant(initial_bias))
])
model.summary()

METRICS=[
    keras.metrics.BinaryAccuracy(name='accuracy'),
    keras.metrics.Precision(name='precision'),
    keras.metrics.Recall(name='recall'),
    keras.metrics.AUC(name='prc', curve='PR'),
    keras.metrics.AUC(name='roc', curve='ROC')
]

model.compile(optimizer=tf.keras.optimizers.Adam(1e-2),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=METRICS)

early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=8,
                                             mode='max',restore_best_weights=True)


history=model.fit(tf_train, validation_data=tf_validation, epochs=40, callbacks=[early_stop])

test_loss,test_acc,test_precision,test_recall,test_pr_curve,test_recall_curve=model.evaluate(tf_test)
print('Test accuracy:', test_acc)
print('Test precision:', test_precision)
print('Test recall:', test_recall)
print('Test PR curve:', test_pr_curve)
print('Test ROC curve:',test_recall_curve)

plot_loss(history)


embedding_dim=128

model=tf.keras.Sequential([
    vectorize_layer,
    Embedding(input_dim=vocab_size,output_dim=embedding_dim,name='embedding',mask_zero=True),
    Bidirectional(LSTM(64)),
    Dense(32,activation='elu'),
    Dense(16,activation='elu'),
    Dense(1,activation='sigmoid',bias_initializer=tf.keras.initializers.Constant(initial_bias))])


METRICS=[
    keras.metrics.BinaryAccuracy(name='accuracy'),
    keras.metrics.Precision(name='precision'),
    keras.metrics.Recall(name='recall'),
    keras.metrics.AUC(name='prc', curve='PR'),
    keras.metrics.AUC(name='roc', curve='ROC')
]

model.compile(optimizer=tf.keras.optimizers.Adam(1e-2),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=METRICS)


early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=8,
                                             mode='max',restore_best_weights=True)

history=model.fit(tf_train, validation_data=tf_validation, epochs=40, callbacks=[early_stop])

test_loss,test_acc,test_precision,test_recall,test_pr_curve,test_recall_curve=model.evaluate(tf_test)
print('Test accuracy:', test_acc)
print('Test precision:', test_precision)
print('Test recall:', test_recall)
print('Test PR curve:', test_pr_curve)
print('Test ROC curve:',test_recall_curve)

plot_loss(history)


embedding_dim=128

model=tf.keras.Sequential([
    vectorize_layer,
    Embedding(input_dim=vocab_size,output_dim=embedding_dim,name='embedding',mask_zero=True),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),
    Dense(16,activation='elu'),
    Dense(1)])

model.summary()
METRICS=[
    keras.metrics.BinaryAccuracy(name='accuracy',threshold=0.0),
    keras.metrics.Precision(name='precision',thresholds=0.0),
    keras.metrics.Recall(name='recall',thresholds=0.0),
    keras.metrics.AUC(name='prc', curve='PR'),
    keras.metrics.AUC(name='roc', curve='ROC')
]



model.compile(optimizer=tf.keras.optimizers.Adam(1e-2),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=METRICS)

early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=8,
                                             mode='max',restore_best_weights=True)


history=model.fit(tf_train, validation_data=tf_validation, epochs=40, callbacks=[early_stop])

test_loss,test_acc,test_precision,test_recall,test_pr_curve,test_recall_curve=model.evaluate(tf_test)
print('Test accuracy:', test_acc)
print('Test precision:', test_precision)
print('Test recall:', test_recall)
print('Test PR curve:', test_pr_curve)
print('Test ROC curve:',test_recall_curve)

plot_loss(history)

import fasttext.util
fasttext.util.download_model('en', if_exists='ignore')  # English
ft = fasttext.load_model('cc.en.300.bin')

#fasttext_model=fasttext.load_model('cc.en.300.bin')
embedding_matrix=np.zeros(shape=(vocab_size,300))

for idx,word in enumerate(vectorize_layer.get_vocabulary()):
  if idx<2:
    continue
  embedding_matrix[idx]=ft.get_word_vector(word)


embedding_dim=300

model=tf.keras.Sequential([
    vectorize_layer,
    Embedding(input_dim=vocab_size,output_dim=embedding_dim, weights=[embedding_matrix],
              name='embedding',mask_zero=True,trainable=False),
    Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),
    Bidirectional(tf.keras.layers.GRU(32)),
    Dense(16,activation='elu'),
    Dense(1)])

model.summary()
METRICS=[
    keras.metrics.BinaryAccuracy(name='accuracy',threshold=0.0),
    keras.metrics.Precision(name='precision',thresholds=0.0),
    keras.metrics.Recall(name='recall',thresholds=0.0),
    keras.metrics.AUC(name='prc', curve='PR'),
    keras.metrics.AUC(name='roc', curve='ROC')
]

model.compile(optimizer=tf.keras.optimizers.Adam(1e-2),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=METRICS)

early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=8,
                                             mode='max',restore_best_weights=True)

validation_fraction = 0.25  # Adjust as needed
num_samples = tf.data.experimental.cardinality(tf_train).numpy()
num_validation_samples = int(validation_fraction * num_samples)

# Create a validation dataset
tf_validation = tf_train.take(num_validation_samples)
tf_train = tf_train.skip(num_validation_samples)


history=model.fit(tf_train, validation_data=tf_validation, epochs=40, callbacks=[early_stop])

test_loss,test_acc,test_precision,test_recall,test_pr_curve,test_recall_curve=model.evaluate(tf_test)
print('Test accuracy:', test_acc)
print('Test precision:', test_precision)
print('Test recall:', test_recall)
print('Test PR curve:', test_pr_curve)
print('Test ROC curve:',test_recall_curve)

plot_loss(history)


model=tf.keras.Sequential([
    vectorize_layer,
    Embedding(input_dim=vocab_size,output_dim=embedding_dim,name='embedding',weights=[embedding_matrix],
              mask_zero=True,trainable=False),
    Bidirectional(LSTM(64)),
    Dense(32,activation='elu'),
    Dense(16,activation='elu'),
    Dense(1,activation='sigmoid',bias_initializer=tf.keras.initializers.Constant(initial_bias))])

model.summary()
METRICS=[
    keras.metrics.BinaryAccuracy(name='accuracy'),
    keras.metrics.Precision(name='precision'),
    keras.metrics.Recall(name='recall'),
    keras.metrics.AUC(name='prc', curve='PR'),
    keras.metrics.AUC(name='roc', curve='ROC')
]


model.compile(optimizer=tf.keras.optimizers.Adam(1e-2),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=METRICS)

early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=8,
                                             mode='max',restore_best_weights=True)

history=model.fit(tf_train, validation_data=tf_validation, epochs=40, callbacks=[early_stop])

test_loss,test_acc,test_precision,test_recall,test_pr_curve,test_recall_curve=model.evaluate(tf_test)
print('Test accuracy:', test_acc)
print('Test precision:', test_precision)
print('Test recall:', test_recall)
print('Test PR curve:', test_pr_curve)
print('Test ROC curve:',test_recall_curve)

plot_loss(history)

